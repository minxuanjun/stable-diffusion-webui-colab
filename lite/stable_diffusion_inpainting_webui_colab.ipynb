{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/minxuanjun/stable-diffusion-webui-colab/blob/main/lite/stable_diffusion_inpainting_webui_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaAJk33ppFw1"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "\n",
        "%env TF_CPP_MIN_LOG_LEVEL=1\n",
        "\n",
        "!apt -y update -qq\n",
        "!wget https://github.com/camenduru/gperftools/releases/download/v1.0/libtcmalloc_minimal.so.4 -O /content/libtcmalloc_minimal.so.4\n",
        "%env LD_PRELOAD=/content/libtcmalloc_minimal.so.4\n",
        "\n",
        "!apt -y install -qq aria2 libcairo2-dev pkg-config python3-dev\n",
        "!pip install -q torch==2.0.1+cu118 torchvision==0.15.2+cu118 torchaudio==2.0.2+cu118 torchtext==0.15.2 torchdata==0.6.1 --extra-index-url https://download.pytorch.org/whl/cu118 -U\n",
        "!pip install -q xformers==0.0.20 triton==2.0.0 gradio_client==0.2.7 -U\n",
        "\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui\n",
        "!git clone https://huggingface.co/embed/negative /content/stable-diffusion-webui/embeddings/negative\n",
        "!git clone https://huggingface.co/embed/lora /content/stable-diffusion-webui/models/Lora/positive\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/embed/upscale/resolve/main/4x-UltraSharp.pth -d /content/stable-diffusion-webui/models/ESRGAN -o 4x-UltraSharp.pth\n",
        "!wget https://raw.githubusercontent.com/camenduru/stable-diffusion-webui-scripts/main/run_n_times.py -O /content/stable-diffusion-webui/scripts/run_n_times.py\n",
        "!git clone -b v2.4 https://github.com/camenduru/deforum-for-automatic1111-webui /content/stable-diffusion-webui/extensions/deforum-for-automatic1111-webui\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui-images-browser /content/stable-diffusion-webui/extensions/stable-diffusion-webui-images-browser\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui-huggingface /content/stable-diffusion-webui/extensions/stable-diffusion-webui-huggingface\n",
        "!git clone -b v2.4 https://github.com/camenduru/sd-civitai-browser /content/stable-diffusion-webui/extensions/sd-civitai-browser\n",
        "!git clone -b v2.4 https://github.com/camenduru/sd-webui-additional-networks /content/stable-diffusion-webui/extensions/sd-webui-additional-networks\n",
        "!git clone -b v2.4 https://github.com/camenduru/sd-webui-tunnels /content/stable-diffusion-webui/extensions/sd-webui-tunnels\n",
        "!git clone -b v2.4 https://github.com/camenduru/batchlinks-webui /content/stable-diffusion-webui/extensions/batchlinks-webui\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui-catppuccin /content/stable-diffusion-webui/extensions/stable-diffusion-webui-catppuccin\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui-rembg /content/stable-diffusion-webui/extensions/stable-diffusion-webui-rembg\n",
        "!git clone -b v2.4 https://github.com/camenduru/stable-diffusion-webui-two-shot /content/stable-diffusion-webui/extensions/stable-diffusion-webui-two-shot\n",
        "!git clone -b v2.4 https://github.com/camenduru/sd-webui-aspect-ratio-helper /content/stable-diffusion-webui/extensions/sd-webui-aspect-ratio-helper\n",
        "!git clone -b v2.4 https://github.com/camenduru/asymmetric-tiling-sd-webui /content/stable-diffusion-webui/extensions/asymmetric-tiling-sd-webui\n",
        "%cd /content/stable-diffusion-webui\n",
        "!git reset --hard\n",
        "!git -C /content/stable-diffusion-webui/repositories/stable-diffusion-stability-ai reset --hard\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/ckpt/i15/resolve/main/sd-v1-5-inpainting.ckpt -d /content/stable-diffusion-webui/models/Stable-diffusion -o sd-v1-5-inpainting.ckpt\n",
        "\n",
        "!sed -i -e '''/from modules import launch_utils/a\\import os''' /content/stable-diffusion-webui/launch.py\n",
        "!sed -i -e '''/        prepare_environment()/a\\        os.system\\(f\\\"\"\"sed -i -e ''\\\"s/dict()))/dict())).cuda()/g\\\"'' /content/stable-diffusion-webui/repositories/stable-diffusion-stability-ai/ldm/util.py\"\"\")''' /content/stable-diffusion-webui/launch.py\n",
        "!sed -i -e 's/\\[\"sd_model_checkpoint\"\\]/\\[\"sd_model_checkpoint\",\"sd_vae\",\"CLIP_stop_at_last_layers\"\\]/g' /content/stable-diffusion-webui/modules/shared.py\n",
        "\n",
        "!python launch.py --listen --xformers --enable-insecure-extension-access --theme dark --gradio-queue --multiple"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import diffusers\n",
        "\n",
        "import torch\n",
        "from diffusers import StableDiffusionPipeline\n",
        "base_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "\n",
        "pipe = StableDiffusionPipeline.from_pretrained(base_model, torch_dtype=torch.float16)"
      ],
      "metadata": {
        "id": "WkkX59aAc9s-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "the pre-trained model includes all the components requireed to setup a complete diffusion pipeline. They are stored in the folloing folders:\n",
        "- `text_encoder`: Stable Diffusion used CLIP, but other diffusion models may use other encoders such as `BERT`.\n",
        "- `tokenizer`: It must match the one used by the `text_encoder` model.\n",
        "- `scheduler`: The scheduling algorithm used to progressively add noise to the image during tranning.\n",
        "- `unet`: The model used to generate the latent representation of the input.\n",
        "- `vae`: Autoencoder module that we'll use to decode latent representations into real images.\n",
        "\n",
        "We can load the components by referring to the folder they were saved, using `subfolder` argument to `from_pretrained`.  "
      ],
      "metadata": {
        "id": "5HWzPyIsNeXl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pipe.components)\n",
        "print(type(pipe.components))\n"
      ],
      "metadata": {
        "id": "J6NBKPeIecoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = pipe.to(\"cuda\")"
      ],
      "metadata": {
        "id": "PnKGVALrsidc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CLip tokenizer [Text Encoder tutorail](https://zhuanlan.zhihu.com/p/680103276)\n",
        "\n",
        "CLIPTokenizer 会将文本拆分成各个单词，然后使用查表法将每个子单词转换为数字。\n",
        "其中每个prompt 编码的最大长度是77。每个编码出的序列都有一些固定的token. *start_token*, *end_token*.\n",
        "```python\n",
        "'tokenizer': CLIPTokenizer(name_or_path='/root/.cache/huggingface/hub/models--runwayml--stable-diffusion-v1-5/snapshots/f03de327dd89b501a01da37fc5240cf4fdba85a1/tokenizer', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'\n",
        "    }, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
        "        49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
        "        49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
        "    }\n",
        "```"
      ],
      "metadata": {
        "id": "b1De2Qjwogg0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = pipe.components[\"tokenizer\"]\n",
        "print(type(tokenizer))\n",
        "prompt = [\"a dog wearning hat\"]\n",
        "prompt = ['tokenizer = pipe.components[\"tokenizer\"]']\n",
        "text_token = tokenizer(prompt, padding=\"max_length\", max_length = tokenizer.model_max_length, truncation=True, return_tensors = \"pt\")\n",
        "text_token = text_token.to(\"cpu\")\n",
        "print(text_token.input_ids.shape)\n",
        "print(text_token)\n",
        "\n",
        "for token in list(text_token.input_ids[0,:7]):\n",
        "  print(f\"{token}:{tokenizer.convert_ids_to_tokens(int(token.item()))}\")"
      ],
      "metadata": {
        "id": "o5J1GLgCr3Ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Encoder\n",
        "Text Encoder 就是将将tokenizer 生成的input_ids 转化为一个文本嵌入的特征，文本嵌入的特征维度是768。Text Encoder 就是， 输出特征维度是[77,768]\n",
        "\n",
        "```python\n",
        "'text_encoder': CLIPTextModel(\n",
        "  (text_model): CLIPTextTransformer(\n",
        "    (embeddings): CLIPTextEmbeddings(\n",
        "      (token_embedding): Embedding(49408,\n",
        "    768)\n",
        "      (position_embedding): Embedding(77,\n",
        "    768)\n",
        "    )\n",
        "    (encoder): CLIPEncoder(\n",
        "      (layers): ModuleList(\n",
        "        (0-11): 12 x CLIPEncoderLayer(\n",
        "          (self_attn): CLIPAttention(\n",
        "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
        "          )\n",
        "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "          (mlp): CLIPMLP(\n",
        "            (activation_fn): QuickGELUActivation()\n",
        "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
        "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
        "          )\n",
        "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "        )\n",
        "      )\n",
        "    )\n",
        "    (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
        "  )\n",
        ")\n",
        "```"
      ],
      "metadata": {
        "id": "NSUdLBsWv1-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_encoder = pipe.components[\"text_encoder\"]\n",
        "# get model structure\n",
        "print(text_encoder)\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embed = text_encoder(text_token.input_ids.to(\"cuda\"))\n",
        "# print(text_embed)\n",
        "print(f\"hidden {text_embed.last_hidden_state.shape}  {text_embed.pooler_output.shape}\")\n"
      ],
      "metadata": {
        "id": "DUIwQcdd6SdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "我们可以通过控制  `num_inference_steps` 来控制去噪次数，去噪次数越多，结果越好。 默认值是50."
      ],
      "metadata": {
        "id": "xP15566OAkWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"a photograph of an astronaut riding a horse\"\n",
        "%debug image = pipe(prompt, num_inference_steps=15).images[0]  # image here is in [PIL format](https://pillow.readthedocs.io/en/stable/)\n",
        "\n",
        "# Now to display an image you can either save it such as:\n",
        "image.save(f\"astronaut_rides_horse.png\")\n",
        "\n",
        "# or if you're in a google colab you can directly display it with\n",
        "image"
      ],
      "metadata": {
        "id": "_Ij_aqPp_VzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The other parameter in the pipeline call is guidance_scale. It is a way to increase the adherence to the conditional signal which in this case is text as well as overall sample quality. In simple terms classifier free guidance forces the generation to better match with the prompt. Numbers like 7 or 8.5 give good results, if you use a very large number the images might look good, but will be less diverse."
      ],
      "metadata": {
        "id": "1Wyk_cHdBR_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To generate multiple images for the same prompt,\n",
        "\n",
        "1.   List item\n",
        "2.   List item\n",
        "\n",
        "we simply use a list with the same prompt repeated several times. We'll send the list to the pipeline instead of the string we used before."
      ],
      "metadata": {
        "id": "8aJbuRHdBw_r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Let's first write a helper function to display a grid of images. Just run the following cell to create the `image_grid` function, or disclose the code if you are interested in how it's done."
      ],
      "metadata": {
        "id": "E_yXGMRxB5C0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "def image_grid(imgs, rows, cols):\n",
        "    assert len(imgs) == rows*cols\n",
        "\n",
        "    w, h = imgs[0].size\n",
        "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
        "    grid_w, grid_h = grid.size\n",
        "\n",
        "    for i, img in enumerate(imgs):\n",
        "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
        "    return grid"
      ],
      "metadata": {
        "id": "KrV44FLxB8cf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now**, we can generate a grid image once having run the pipeline with a list of 3 prompts."
      ],
      "metadata": {
        "id": "z1R_Ly6OCA1S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_images = 12\n",
        "prompt = [\"a photograph of an astronaut riding a horse\"] * num_images\n",
        "\n",
        "images = pipe(prompt).images\n",
        "\n",
        "grid = image_grid(images, rows=4, cols=3)\n",
        "grid"
      ],
      "metadata": {
        "id": "NYz1jKYKCAbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is Stable Diffusion\n",
        "Now, let's go into the theoretiacal part of Stable Diffusion.\n",
        "\n",
        "Stable Diffusion is based on a particular type of diffusion model call **Latent Diffusion**, proposed in [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752).\n"
      ],
      "metadata": {
        "id": "UY-qpi7OXi0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "General diffusion models are machine learning systems that are trained to denoise random gaussian noise step by step, to get to a sample of interest, such as an image, For a more detaied overview of how they work, check [this colab](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb).\n",
        "\n",
        "Diffusion models have shown to achieve state-of-the-art results for generating image data, But one downside of diffusion models is that the reverse denoising process is slow. In addition, these models consume a lot of memory because they operate in pixel space, which becomes unreasonably expensive when generating high-resolution images. Therefore, it is challenging to tain these models and also use them for inference."
      ],
      "metadata": {
        "id": "5wPsndZn6Trz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent diffusion can reduce memory and compute complexity by applying the diffusion process over a lower dimensional latent space, instead of using the actual pixel space. This is the key difference between standard diffusion and latent diffusion models: in latent diffusion the model is trained to generate latent (compressed) representations of the images.\n",
        "\n",
        "There are three main components in latent diffusion.\n",
        "\n",
        "1. An autoencoder (VAE).\n",
        "2. A [U-Net](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/diffusers_intro.ipynb#scrollTo=wW8o1Wp0zRkq).\n",
        "3. A text-encoder, *e.g.* [CLIP's Text Encoder](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel).\n"
      ],
      "metadata": {
        "id": "BvnSacsa31rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. The autoencoder(VAE)**\n",
        "\n",
        "The VAE model has two parts, an encoder and a decoder. The encoder is used to convert the image into a low dimensional latent representation, which will serve as the input to the *U-Net* model.\n",
        "Tne decoder, conversely, transforms the laten representation back into an image.\n",
        "\n",
        "During latent diffusion _training_, the encoder is used to get the latent representation (_latens_) of the images for the forward diffusion process, which applies more and more noise at each step.\n",
        "During _inference_, the denoised latents generated by the reverse diffusion process are converted back into images using the VAE decoder. As will see during inference we only need the VAE dencoder.\n"
      ],
      "metadata": {
        "id": "c-WQMfaw6IYl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. The U-Net**\n",
        "The U-Net has encoder part and decoder part both comprised of ResNet blocks. The encoder compresses an image representation into a lower resolution image representation and the decoder decodes the lower resolution image representation back to the original higher image representation that is supposedly less noisy. More specifically, the U-Net output predicts the *noise residual* which can be used to compute the denoised image representation.\n",
        "\n",
        "To prevent the U-Net from losing important information while downsampling, short-cut connetctions are usuallig added between the downsampling ResNets of the encoder to the upsampling ResNets of the decoder.\n",
        "Additionally, the stable diffusion U-Net is able to condition its output on text-enbeddding via cross-attention layers. The cross-attention layers are added to both the encoder and decoder part of the U-Net usually between ResNet blocks."
      ],
      "metadata": {
        "id": "jp3w-oG-84Z4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. The Text-endoer**\n",
        "The text-encoder is reponsible for transformiing the input prompt, *e.g.\" \"An astronout riding a horse\" into an embedding space that can be understand by the U-Net. It is usually  a simple *transformer-based* encoder that maps a sequnce of input tokens to a sequnce of latent text-embeddings.\n",
        "\n",
        "Inspired by [Image](https://imagen.research.google/), Stable Diffusion does **not** train the text-encoder during trainning and simply use an CLIP's already trained text encoder,  [CLIPTextModel](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPTextModel)."
      ],
      "metadata": {
        "id": "QnjFGZuZ_n1K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why is latent diffusion fast and efficient?**\n",
        "\n",
        "Since the U-Net latent diffusion models operators on a low dimensional space, it greatly reducesthe memory and compute requirements compared to pixel-spce diffusion models."
      ],
      "metadata": {
        "id": "a-pSohk4GZhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How to write your own inference pipeline with 'diffusers'\n",
        "\n",
        "Finally, we show how you can create custom diffusion pipelines with `diffusers`.\n",
        "In this section, we will demonstrate how to use Stable Diffusion with a different scheduler, namely [Katherine Crowson's](https://github.com/crowsonkb) K-LMS scheduler that was added in [this PR](https://github.com/huggingface/diffusers/pull/185#pullrequestreview-1074247365)."
      ],
      "metadata": {
        "id": "UUFlgCe7HtaD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's go through the `StableDiffusionPipeline` step by step to see how we cloud have written its ourselves.\n",
        "We will start by loading the individual models involved.  "
      ],
      "metadata": {
        "id": "xH1mHVn1TdGV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "from diffusers import AutoencoderKL, UNet2DConditionModel, PNDMScheduler\n",
        "torch_device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "base_model = \"runwayml/stable-diffusion-v1-5\"\n",
        "torch_dtype = torch.float32\n",
        "# 1. Load the autoencoder model with will be used to decode the latents into image space.\n",
        "vae = AutoencoderKL.from_pretrained(base_model, subfolder=\"vae\", torch_dtype= torch_dtype)\n",
        "# 2. Load the tokenizer and text encoder to tokenizer and encode the text.\n",
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype = torch_dtype)\n",
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\", torch_dtype= torch_dtype)\n",
        "\n",
        "# 3. The UNet model for generating the latents\n",
        "unet =UNet2DConditionModel.from_pretrained(base_model, subfolder='unet', torch_dtype=torch_dtype)"
      ],
      "metadata": {
        "id": "VI1GSfLE5Lk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now instead of loading the pre-defined scheduler, we'll use the K-LMS scheduler instead."
      ],
      "metadata": {
        "id": "wj7FwwS-XRFU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import LMSDiscreteScheduler\n",
        "\n",
        "scheduler = LMSDiscreteScheduler.from_pretrained(base_model, subfolder=\"scheduler\", torch_dtype=torch_dtype)"
      ],
      "metadata": {
        "id": "cgzLKdInXiDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we move the models to the GPU."
      ],
      "metadata": {
        "id": "5LOxS8HSaaY0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vae = vae.to(torch_device)\n",
        "text_encoder = text_encoder.to(torch_device)\n",
        "unet= unet.to(torch_device)"
      ],
      "metadata": {
        "id": "1-uB37jhagfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now define the parameters we'll use to generate images.\n",
        "Note that `guidance_scale = 1` corresponds to doing no classifier-free guaidance.\n",
        "Here we set it to7.5 as also done previously.\n",
        "In contrast to the previous examples, we set `num_inference_steps` to 100 to get an even more defined image."
      ],
      "metadata": {
        "id": "RbHmih2Ma9Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\"a photograph of an astronaut riding a horse\"]\n",
        "\n",
        "height = 512\n",
        "width= 512\n",
        "\n",
        "num_inference_steps = 100\n",
        "guidance_scale = 7.5\n",
        "generator = torch.manual_seed(32)\n",
        "\n",
        "batch_size = 1\n"
      ],
      "metadata": {
        "id": "D8nMG6p0fGi2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we get the text_embedding for the prompt. These embeddingswill be used to condition the UNet model."
      ],
      "metadata": {
        "id": "yj2oQEnRfk_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  text_embeddings = text_encoder(text_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "DQ5xUQKSf_Kx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also get the unconditional text embedding for classifier-free guidance, which are just the embedding for the padding token(empty text).\n",
        "They need to have the same shape as the conditional `text_embeddings` (`batch_size` and `seq_length`)"
      ],
      "metadata": {
        "id": "36Jqplu0gURY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length =text_input.input_ids.shape[-1]\n",
        "print(max_length)\n",
        "uncond_input = tokenizer([\"\"]* batch_size, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "with torch.no_grad():\n",
        "  uncond_embeddings = text_encoder(uncond_input.input_ids.to(torch_device))[0]"
      ],
      "metadata": {
        "id": "-iF2s9mBi1Lx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For classifier-free guidance, we need to do two forward passes. Once with the conditioned input(`text_embeddings`), and another with unconditional embedding (`uncond_embeddings`) In practice, we can concatenate both into a single batch to avoid doing two forward passes."
      ],
      "metadata": {
        "id": "oOXm9OQGj2Ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_embeddings= torch.cat([uncond_embeddings, text_embeddings])\n",
        "print(text_embeddings.shape)"
      ],
      "metadata": {
        "id": "cYx3o86ulO-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate the Initial random noise"
      ],
      "metadata": {
        "id": "VPMuEQ4slihF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latents = torch.randn((batch_size, unet.config.in_channels, height//8, width//8))\n",
        "print(latents.shape)"
      ],
      "metadata": {
        "id": "HOqZ7ffJlmem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cool $64 \\times 64$ is expected. The model will transform this latent representation (pure noise) into a `512 × 512` image later on.\n",
        "\n",
        "Next, we initialize the scheduler with our chosen `num_inference_steps`.\n",
        "This will compute the `sigmas` and exact time step values to be used during the denoising process.\n"
      ],
      "metadata": {
        "id": "yAh4M1HcmIkq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.set_timesteps(num_inference_steps)"
      ],
      "metadata": {
        "id": "14zYHh4dmS3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The K-LMS scheduler needs to multiply the `latents` by its `sigma` values. Let's do this here"
      ],
      "metadata": {
        "id": "fT3jD5kMmchn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "latents = latents * scheduler.init_noise_sigma"
      ],
      "metadata": {
        "id": "2a_kyziPmetg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are ready to write the denoising loop."
      ],
      "metadata": {
        "id": "8lujxYwdmiw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm\n",
        "from torch import autocast\n",
        "\n",
        "for t in tqdm(scheduler.timesteps):\n",
        "  # expand the latents if we are doing classifier-free guidance to avoid doing two forward passes.\n",
        "  latent_model_input = torch.cat([latents] * 2)\n",
        "\n",
        "  latent_model_input = scheduler.scale_model_input(latent_model_input, t)\n",
        "\n",
        "  # predict the noise residual\n",
        "  with torch.no_grad():\n",
        "    noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "  # perform guidance\n",
        "  noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "  noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
        "\n",
        "  # compute the previous noisy sample x_t -> x_t-1\n",
        "  latents = scheduler.step(noise_pred, t, latents).prev_sample"
      ],
      "metadata": {
        "id": "kG8mfnpNmkmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now use the `vae` to decode the generated `latents` back into the image."
      ],
      "metadata": {
        "id": "VwtldfY-m659"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# scale and decode the image latents with vae\n",
        "latents = 1 / 0.18215 * latents\n",
        "\n",
        "with torch.no_grad():\n",
        "  image = vae.decode(latents).sample"
      ],
      "metadata": {
        "id": "qw8u0F4pm_vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And finally, let's convert the image to PIL so we can display or save it."
      ],
      "metadata": {
        "id": "yXrP8mWqnCUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "images = (image * 255).round().astype(\"uint8\")\n",
        "pil_images = [Image.fromarray(image) for image in images]\n",
        "pil_images[0]"
      ],
      "metadata": {
        "id": "_M78LGMlnKek"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}